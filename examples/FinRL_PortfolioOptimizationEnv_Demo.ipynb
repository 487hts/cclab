{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xt6fIDownZs"
      },
      "source": [
        "# A guide Portfolio Optimization Environment\n",
        "\n",
        "This notebook aims to provide an example of using PortfolioOptimizationEnv (or POE) to train a reinforcement learning model that learns to solve the portfolio optimization problem.\n",
        "\n",
        "In this document, we will reproduce a famous architecture called EIIE (ensemble of identical independent evaluators), introduced in the following paper:\n",
        "\n",
        "- Zhengyao Jiang, Dixing Xu, & Jinjun Liang. (2017). A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem. https://doi.org/10.48550/arXiv.1706.10059.\n",
        "\n",
        "It's advisable to read it to understand the algorithm implemented in this notebook.\n",
        "\n",
        "### Note\n",
        "If you're using this environment, consider citing the following paper (in adittion to FinRL references):\n",
        "\n",
        "- Caio Costa, & Anna Costa (2023). POE: A General Portfolio Optimization Environment for FinRL. In *Anais do II Brazilian Workshop on Artificial Intelligence in Finance* (pp. 132–143). SBC. https://doi.org/10.5753/bwaif.2023.231144.\n",
        "\n",
        "```\n",
        "@inproceedings{bwaif,\n",
        " author = {Caio Costa and Anna Costa},\n",
        " title = {POE: A General Portfolio Optimization Environment for FinRL},\n",
        " booktitle = {Anais do II Brazilian Workshop on Artificial Intelligence in Finance},\n",
        " location = {João Pessoa/PB},\n",
        " year = {2023},\n",
        " keywords = {},\n",
        " issn = {0000-0000},\n",
        " pages = {132--143},\n",
        " publisher = {SBC},\n",
        " address = {Porto Alegre, RS, Brasil},\n",
        " doi = {10.5753/bwaif.2023.231144},\n",
        " url = {https://sol.sbc.org.br/index.php/bwaif/article/view/24959}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0L7FZeWMUHp"
      },
      "source": [
        "## Installation and imports\n",
        "\n",
        "To run this notebook in google colab, uncomment the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XGHfTt1HMVQw"
      },
      "outputs": [],
      "source": [
        "# install finrl library\n",
        "!sudo apt install swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-GLganWiMYZ1"
      },
      "outputs": [],
      "source": [
        "# We also need to install quantstats, because the environment uses it to plot graphs\n",
        "!pip install quantstats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6RqrzokqoanP"
      },
      "outputs": [],
      "source": [
        "# Hide matplotlib warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "logging.getLogger('matplotlib.font_manager').disabled = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "cP5t6U7-nYoc",
        "outputId": "85867a12-987d-4325-aa2e-a796f8b1d842"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'finrl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1499636483>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxAbsScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myahoodownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYahooDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupByScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_portfolio_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_portfolio_optimization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPortfolioOptimizationEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'finrl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.meta.preprocessor.preprocessors import GroupByScaler\n",
        "from finrl.meta.env_portfolio_optimization.env_portfolio_optimization import PortfolioOptimizationEnv\n",
        "from finrl.agents.portfolio_optimization.models import DRLAgent\n",
        "from finrl.agents.portfolio_optimization.architectures import EIIE\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY2yhvpASEyo"
      },
      "source": [
        "## Fetch data\n",
        "\n",
        "In his paper, *Jiang et al* creates a portfolio composed by the top-11 cryptocurrencies based on 30-days volume. Since it's not specified when this classification was done, it's difficult to reproduce, so we will use a similar approach in the Brazillian stock market:\n",
        "\n",
        "- We select top-10 stocks from Brazillian stock market;\n",
        "- For simplicity, we disconsider stocks that have missing data for the days in period 2011-01-01 to 2019-12-31 (9 years);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H11UjCstSFwm"
      },
      "outputs": [],
      "source": [
        "TOP_BRL = [\n",
        "    \"VALE3.SA\", \"PETR4.SA\", \"ITUB4.SA\", \"BBDC4.SA\",\n",
        "    \"BBAS3.SA\", \"RENT3.SA\", \"LREN3.SA\", \"PRIO3.SA\",\n",
        "    \"WEGE3.SA\", \"ABEV3.SA\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkm96aNsSIji"
      },
      "outputs": [],
      "source": [
        "print(len(TOP_BRL))\n",
        "\n",
        "portfolio_raw_df = YahooDownloader(start_date = '2011-01-01',\n",
        "                                end_date = '2022-12-31',\n",
        "                                ticker_list = TOP_BRL).fetch_data()\n",
        "portfolio_raw_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UqpIXsuSKfO"
      },
      "outputs": [],
      "source": [
        "portfolio_raw_df.groupby(\"tic\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quy6DiCRcygO"
      },
      "source": [
        "### Normalize Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxqvfNCucygO"
      },
      "source": [
        "We normalize the data dividing the time series of each stock by its maximum value, so that the dataframe contains values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piAgvTE5cygO"
      },
      "outputs": [],
      "source": [
        "portfolio_norm_df = GroupByScaler(by=\"tic\", scaler=MaxAbsScaler).fit_transform(portfolio_raw_df)\n",
        "portfolio_norm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNtJN_LPcygO"
      },
      "outputs": [],
      "source": [
        "df_portfolio = portfolio_norm_df[[\"date\", \"tic\", \"close\", \"high\", \"low\"]]\n",
        "\n",
        "df_portfolio_train = df_portfolio[(df_portfolio[\"date\"] >= \"2011-01-01\") & (df_portfolio[\"date\"] < \"2019-12-31\")]\n",
        "df_portfolio_2020 = df_portfolio[(df_portfolio[\"date\"] >= \"2020-01-01\") & (df_portfolio[\"date\"] < \"2020-12-31\")]\n",
        "df_portfolio_2021 = df_portfolio[(df_portfolio[\"date\"] >= \"2021-01-01\") & (df_portfolio[\"date\"] < \"2021-12-31\")]\n",
        "df_portfolio_2022 = df_portfolio[(df_portfolio[\"date\"] >= \"2022-01-01\") & (df_portfolio[\"date\"] < \"2022-12-31\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM829994GWo3"
      },
      "source": [
        "### Instantiate Environment\n",
        "\n",
        "Using the `PortfolioOptimizationEnv`, it's easy to instantiate a portfolio optimization environment for reinforcement learning agents. In the example below, we use the dataframe created before to start an environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uj9CqamcygO"
      },
      "outputs": [],
      "source": [
        "environment = PortfolioOptimizationEnv(\n",
        "        df_portfolio_train,\n",
        "        initial_amount=100000,\n",
        "        comission_fee_pct=0.0025,\n",
        "        time_window=50,\n",
        "        features=[\"close\", \"high\", \"low\"],\n",
        "        normalize_df=None\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTB4UHpmcygP"
      },
      "source": [
        "### Instantiate Model\n",
        "\n",
        "Now, we can instantiate the model using FinRL API. In this example, we are going to use the EIIE architecture introduced by Jiang et. al.\n",
        "\n",
        ":exclamation: **Note:** Remember to set the architecture's `time_window` parameter with the same value of the environment's `time_window`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr82W3E0uQSo",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# set PolicyGradient parameters\n",
        "model_kwargs = {\n",
        "    \"lr\": 0.01,\n",
        "    \"policy\": EIIE,\n",
        "}\n",
        "\n",
        "# here, we can set EIIE's parameters\n",
        "policy_kwargs = {\n",
        "    \"k_size\": 3,\n",
        "    \"time_window\": 50,\n",
        "}\n",
        "\n",
        "model = DRLAgent(environment).get_model(\"pg\", device, model_kwargs, policy_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoairkDjcygP"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt1-iljxcygP"
      },
      "outputs": [],
      "source": [
        "DRLAgent.train_model(model, episodes=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE7X3qEeXOr4"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcWuPgPvXNpP"
      },
      "outputs": [],
      "source": [
        "torch.save(model.train_policy.state_dict(), \"policy_EIIE.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FRK9A98XVck"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFYB9iGwAPSh"
      },
      "source": [
        "### Instantiate different environments\n",
        "\n",
        "Since we have three different periods of time, we need three different environments instantiated to simulate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhsL5Cxx9d5s"
      },
      "outputs": [],
      "source": [
        "environment_2020 = PortfolioOptimizationEnv(\n",
        "    df_portfolio_2020,\n",
        "    initial_amount=100000,\n",
        "    comission_fee_pct=0.0025,\n",
        "    time_window=50,\n",
        "    features=[\"close\", \"high\", \"low\"],\n",
        "    normalize_df=None\n",
        ")\n",
        "\n",
        "environment_2021 = PortfolioOptimizationEnv(\n",
        "    df_portfolio_2021,\n",
        "    initial_amount=100000,\n",
        "    comission_fee_pct=0.0025,\n",
        "    time_window=50,\n",
        "    features=[\"close\", \"high\", \"low\"],\n",
        "    normalize_df=None\n",
        ")\n",
        "\n",
        "environment_2022 = PortfolioOptimizationEnv(\n",
        "    df_portfolio_2022,\n",
        "    initial_amount=100000,\n",
        "    comission_fee_pct=0.0025,\n",
        "    time_window=50,\n",
        "    features=[\"close\", \"high\", \"low\"],\n",
        "    normalize_df=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4RuS2pRAa4H"
      },
      "source": [
        "### Test EIIE architecture\n",
        "Now, we can test the EIIE architecture in the three different test periods. It's important no note that, in this code, we load the saved policy even though it's not necessary just to show how to save and load your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeRy__TI9CAs"
      },
      "outputs": [],
      "source": [
        "EIIE_results = {\n",
        "    \"training\": environment._asset_memory[\"final\"],\n",
        "    \"2020\": {},\n",
        "    \"2021\": {},\n",
        "    \"2022\": {}\n",
        "}\n",
        "\n",
        "# instantiate an architecture with the same arguments used in training\n",
        "# and load with load_state_dict.\n",
        "policy = EIIE(time_window=50, device=device)\n",
        "policy.load_state_dict(torch.load(\"policy_EIIE.pt\"))\n",
        "\n",
        "# 2020\n",
        "DRLAgent.DRL_validation(model, environment_2020, policy=policy)\n",
        "EIIE_results[\"2020\"][\"value\"] = environment_2020._asset_memory[\"final\"]\n",
        "\n",
        "# 2021\n",
        "DRLAgent.DRL_validation(model, environment_2021, policy=policy)\n",
        "EIIE_results[\"2021\"][\"value\"] = environment_2021._asset_memory[\"final\"]\n",
        "\n",
        "# 2022\n",
        "DRLAgent.DRL_validation(model, environment_2022, policy=policy)\n",
        "EIIE_results[\"2022\"][\"value\"] = environment_2022._asset_memory[\"final\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZc5PpbaBU-J"
      },
      "source": [
        "### Test Uniform Buy and Hold\n",
        "For comparison, we will also test the performance of a uniform buy and hold strategy. In this strategy, the portfolio has no remaining cash and the same percentage of money is allocated in each asset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntHO_UIs-83T"
      },
      "outputs": [],
      "source": [
        "UBAH_results = {\n",
        "    \"train\": {},\n",
        "    \"2020\": {},\n",
        "    \"2021\": {},\n",
        "    \"2022\": {}\n",
        "}\n",
        "\n",
        "PORTFOLIO_SIZE = len(TOP_BRL)\n",
        "\n",
        "# train period\n",
        "terminated = False\n",
        "environment.reset()\n",
        "while not terminated:\n",
        "    action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
        "    _, _, terminated, _ = environment.step(action)\n",
        "UBAH_results[\"train\"][\"value\"] = environment._asset_memory[\"final\"]\n",
        "\n",
        "# 2020\n",
        "terminated = False\n",
        "environment_2020.reset()\n",
        "while not terminated:\n",
        "    action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
        "    _, _, terminated, _ = environment_2020.step(action)\n",
        "UBAH_results[\"2020\"][\"value\"] = environment_2020._asset_memory[\"final\"]\n",
        "\n",
        "# 2021\n",
        "terminated = False\n",
        "environment_2021.reset()\n",
        "while not terminated:\n",
        "    action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
        "    _, _, terminated, _ = environment_2021.step(action)\n",
        "UBAH_results[\"2021\"][\"value\"] = environment_2021._asset_memory[\"final\"]\n",
        "\n",
        "# 2022\n",
        "terminated = False\n",
        "environment_2022.reset()\n",
        "while not terminated:\n",
        "    action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
        "    _, _, terminated, _ = environment_2022.step(action)\n",
        "UBAH_results[\"2022\"][\"value\"] = environment_2022._asset_memory[\"final\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBMM7hAHC6rq"
      },
      "source": [
        "### Plot graphics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8YrDNpeC71w"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(UBAH_results[\"train\"][\"value\"], label=\"Buy and Hold\")\n",
        "plt.plot(EIIE_results[\"training\"], label=\"EIIE\")\n",
        "\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Portfolio Value\")\n",
        "plt.title(\"Performance in training period\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQniascoDIH2"
      },
      "outputs": [],
      "source": [
        "plt.plot(UBAH_results[\"2020\"][\"value\"], label=\"Buy and Hold\")\n",
        "plt.plot(EIIE_results[\"2020\"][\"value\"], label=\"EIIE\")\n",
        "\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Portfolio Value\")\n",
        "plt.title(\"Performance in 2020\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hJtnW7QDIt2"
      },
      "outputs": [],
      "source": [
        "plt.plot(UBAH_results[\"2021\"][\"value\"], label=\"Buy and Hold\")\n",
        "plt.plot(EIIE_results[\"2021\"][\"value\"], label=\"EIIE\")\n",
        "\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Portfolio Value\")\n",
        "plt.title(\"Performance in 2021\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hJD79w-DJXo"
      },
      "outputs": [],
      "source": [
        "plt.plot(UBAH_results[\"2022\"][\"value\"], label=\"Buy and Hold\")\n",
        "plt.plot(EIIE_results[\"2022\"][\"value\"], label=\"EIIE\")\n",
        "\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Portfolio Value\")\n",
        "plt.title(\"Performance in 2022\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP6WVgalcygQ"
      },
      "source": [
        "We can see that the agent is able to learn a good policy but its performance is worse the more the test period advances into the future. To get a better performance in 2022, for example, the agent should probably be trained again using more recent data."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}